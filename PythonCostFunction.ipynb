{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Index](https://github.com/basilhan/ml-concepts/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Recall that a machine learning model is described mathematically by its [hypothesis function](https://github.com/basilhan/ml-concepts/blob/master/PythonHypothesisFunction.ipynb) $h_{\\mathbf{p}}(\\mathbf{x})$. This function is parameterized by a set of parameters $\\mathbf{p}$. In other words, there can be as many possible versions of a selected model as the set of all possible permutations of parameters. Obviously, the versions will be of different quality. Here we need to define what makes a version <i>good</i>. It is for this purpose that we introduce the cost function.  \n",
    "<br>\n",
    "The cost function $J(\\mathbf{p})$ is a measure of the errors in the model. Mathematically, given a dataset it maps each version of a model (as defined by the parameters $\\mathbf{p}$) to a positive real number. This is an aggregated score of the deviations of the learned values from the actual values when the model is applied to the dataset. There exists a set of parameters $\\mathbf{p}_{opt}$ which returns the minimal value of $J(\\mathbf{p})$. We are interested in determining $\\mathbf{p}_{opt}$.  \n",
    "<br>\n",
    "But first, we need to defined $J(\\mathbf{p})$. This is determined by the specific model selected. We next turn our attention to each of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locating the Minimum of the Cost Function\n",
    "\n",
    "As mentioned, once we have defined our $J(\\mathbf{p})$, we seek the set of parameters $\\mathbf{p}_{opt}$ which returns the minimal value. We could try out different parameters randomly and observe what $J(\\mathbf{p})$ returns and then choose the one which is smallest. But considering the size of the parameter space in a typical machine learning task, locating the optimal set by chance would be hugely improbable. It would be good to have a more systematic way of accomplishing this. A popular technique employed is the [gradient descent](https://github.com/basilhan/ml-concepts/blob/master/PythonGradientDescent.ipynb) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Permalink : https://nbviewer.jupyter.org/github/basilhan/ml-concepts/blob/master/PythonCostFunction.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
