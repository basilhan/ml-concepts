{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Index](https://github.com/basilhan/ml-concepts/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "<font color=\"blue\">Gradient descent</font> is an optimization algorithm used to find the values of parameters that give a local minimum of the cost function over a dataset.\n",
    "It works by iteratively updating the parameters in the <i>opposite</i> direction of the gradient of the cost function $\\nabla{J(\\mathbf{p})}$ (a vector of the partial derivatives) w.r.t. the parameters. \n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla{J(\\mathbf{p})}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        \\frac{\\partial}{\\partial p_0} J(\\mathbf{p}) \\\\\n",
    "        \\frac{\\partial}{\\partial p_0} J(\\mathbf{p}) \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{\\partial}{\\partial p_j} J(\\mathbf{p}) \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{\\partial}{\\partial p_n} J(\\mathbf{p})\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The learning rate $\\eta$ (a hyperparameter) determines the size of the step to take at each iteration, also known as an epoch. In other words, we allow ourselves to \"slide\" down the slope of the $(n+1)$-dimensional surface defined by $J(\\mathbf{p})$ until we reach the bottom of the \"valley\". If successful we call this process convergence.\n",
    "\n",
    "\\begin{equation}\n",
    "p_j := p_j - \\eta\\frac{\\partial}{\\partial p_j} J(\\mathbf{p})\n",
    "\\end{equation}\n",
    "\n",
    "for $j = 1, 2, \\cdots, n$\n",
    "\n",
    "We will unpack this expression in the next few sections.\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/GradientDescent.png?raw=true\">\n",
    "\n",
    "The graph shows a simplified cost function of a single parameter with a minimum $J_{min}$ located at where the cross is. Although it only shows one parameter (a particular weight or the bias), the exact same procedure is followed and repeated independently for every parameter (i.e. for every $p_j$ where $j = 1$ to $n$).\n",
    "\n",
    "#### Direction of Update\n",
    "\n",
    "Consider the case where we are on the left of $p_{opt}$ (e.g. the red dot). Here the gradient is negative and we note that we need to increase $p$ to reach $p_{opt}$. Conversely, when we are on the right of $p_{opt}$ (e.g. the green or orange dot), the gradient is positive and we need to decrease $p$. This means that we always need to minus off from $p$ a quantity of the same sign as the gradient.\n",
    "\n",
    "$$p_j := p_j \\color{red}{-} \\eta\\frac{\\partial}{\\partial p_j} J(\\mathbf{p})$$\n",
    "\n",
    "#### Magnitude of Update\n",
    "\n",
    "Where the absolute value of the gradient is large, it makes intuitive sense that there is still a distance from the bottom of the valley where the gradient is zero. We can afford to take a bigger step to quicken the journey. As we get closer to our target, the gradient approaches zero and it would be prudent to reduce the step size. Otherwise, we might oscillate forever around our target. Therefore, we make our step size proportional to the partial derivative.\n",
    "\n",
    "$$p_j := p_j - \\eta\\color{red}{\\frac{\\partial}{\\partial p_j} J(}\\mathbf{p} \\color{red}{)}$$\n",
    "\n",
    "#### Learning Rate\n",
    "\n",
    "The learning rate is an important hyperparameter which allows us direct control over the update step size. If the step size is too small, more computation is needed as convergence will be slow. If the step size is too large, we risk jumping over the targeted optimal and oscillate about it or in the worst case even result in divergence.\n",
    "\n",
    "$$p_j := p_j - \\color{red}{\\eta}\\frac{\\partial}{\\partial p_j} J(\\mathbf{p})$$\n",
    "\n",
    "#### Simultaneity of Updates\n",
    "\n",
    "For multi-dimensional parameter space, it is important that the same $\\nabla{J(\\mathbf{p})}$ is used for all parameters in the same update cycle. Since $\\nabla{J(\\mathbf{p})}$ is dependent on $\\mathbf{p} = \\begin{bmatrix} p_1 & p_2 & \\cdots & p_n \\end{bmatrix}^\\top$ and we are calculating a new value for each $p_j$, we need to take care to introduce the updates only after all new values have been obtained.\n",
    "\n",
    "$$\\vdots$$\n",
    "$$p_j := p_j - \\eta\\frac{\\partial}{\\partial p_j} J(\\color{red}{\\mathbf{p}})$$\n",
    "$$p_{j+1} := p_{j+1} - \\eta\\frac{\\partial}{\\partial p_{j+1}} J(\\color{red}{\\mathbf{p}})$$\n",
    "$$\\vdots$$\n",
    "\n",
    "#### Global Minimum\n",
    "\n",
    "If the cost function is [convex](https://en.wikipedia.org/wiki/Convex_function), there is a global minimum and hence no localized \"dips\" in the surface of the function which might give spurious results. This is the case for linear regression and logistic regression. However, this is not so in general.\n",
    "\n",
    "#### Termination Criteria\n",
    "\n",
    "_Convergence_\n",
    "\n",
    "_Number of Epochs_\n",
    "\n",
    "#### Variants\n",
    "\n",
    "[Recall](https://nbviewer.jupyter.org/github/basilhan/ml-concepts/blob/master/PythonCostFunction.ipynb) that we have defined the cost function as an average score of the deviations of the learned values from the actual values over the entire set of training data instances available to us.\n",
    "\n",
    "$$\n",
    "J:\\mathbf{p} \\in \\mathbb{R}^n \\mapsto \\mathbb{R} \\text{ for} \\begin{bmatrix} \\mathbf{x}^{(1)} \\cdots \\mathbf{x}^{(m)} \\end{bmatrix} \\\\\n",
    "$$\n",
    "\n",
    "It turns out that there is some flexibility in the set of data instances to use in averaging during each epoch for new parameter calculation and this gives rise to a few variants.\n",
    "\n",
    "_Batch Gradient Descent_\n",
    "\n",
    "This is the plain vanilla variant as defined above (i.e. the entire dataset is used). Some characteristics :\n",
    "\n",
    "* Slow. Computations need to be done on every data instance before a single update to the parameters can be made.\n",
    "* Memory intensive. This is an efficiency issue if the dataset does not fit into memory and data have to be fetched repeatedly from disk.\n",
    "* Inability to allow on-the-fly update of data instances (what we describe as being \"online\").\n",
    "* With enough computation, convergence to the global minimum is guaranteed for convex cost functions and to a local minimum for non-convex ones.\n",
    "\n",
    "_Stochastic Gradient Descent_\n",
    "\n",
    "This variant (also knowned as SGD) takes the other extreme from batch gradient descent. Only a single data instance is used to make the parameter update each epoch. This hastens the update cycle but also introduces a few considerations :\n",
    "\n",
    "* The variance of the change introduced in each epoch is higher (hence the name) than for batch gradient descent. While in batch gradient descent the trajectory of the search through parameter space is relatively smooth, in SGD it tends to look more chaotic.\n",
    "* This higher variance actually allows jumps out of shallow local minima into other regions with deeper minima.\n",
    "* A smaller learning rate is typically chosen for SGD to encourage convergence. It has also been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. \n",
    "* To reduce bias, the dataset is shuffled at each epoch.\n",
    "* SGD is sensitive to feature scaling.\n",
    "\n",
    "_Mini-batch Gradient Descent_\n",
    "\n",
    "This variant occupies the large middle group between batch and stochastic gradient descent. To compute a single update, a set size of greater than one and less than the number of available data instances is used. Mini-batch gradient descent offers greater convergence efficiency than batch gradient descent with less variance than stochastic gradient descent. Because computation takes place in a batch, highly optimized matrix operations can be taken advantage of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Permalink : https://nbviewer.jupyter.org/github/basilhan/ml-concepts/blob/master/PythonGradientDescent.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
