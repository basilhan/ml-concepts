{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Index](https://github.com/basilhan/ml-concepts/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "A hypothesis function $h_{\\mathbf{p}}(\\mathbf{x})$ provides a mathematical description of a machine learning model by mapping the feature vector $\\mathbf{x} = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_n \\end{bmatrix}^\\top$ to a real value that will eventually be converted to the final learned target value. The subscript in $h_{\\mathbf{p}}(\\mathbf{x})$ indicates that the function is parameterized by the parameter vector $\\mathbf{p}$.\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/HypothesisFunction.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models\n",
    "\n",
    "Many types of machine learning models are [linearly](https://en.wikipedia.org/wiki/Linear_function) parameterized. In these models, the parameter vector consists of a a scalar $b$ known as the bias, and a weight vector $\\mathbf{w}$. Mathematically, the parameter vector can be expressed as :\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{p}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        p_0 \\\\\n",
    "        p_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        p_n\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        b \\\\\n",
    "        w_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        w_n\n",
    "    \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "where $b = p_0$ and $w_j = p_j$ for $j = 1, 2, \\cdots, n$. Therefore, the weight vector, when fully expanded, is :\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{w}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        w_1 \\\\\n",
    "        w_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        w_n\n",
    "    \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the value of $n$ is also the number of dimensions in the feature space. There is therefore a one-to-one correspondence between the weight vector and the feature vector $\\begin{bmatrix} x_1 & x_2 & \\cdots & x_n \\end{bmatrix}^\\top$. That is, the weight $w_j$ corresponds to the feature $x_j$, where $j$ is an integer from $1$ to $n$. We can now write out the following linear expression.\n",
    "\n",
    "\\begin{equation}\n",
    "b + w_1x_1 + w_2x_2 + \\cdots + w_nx_n\n",
    "\\end{equation}\n",
    "\n",
    "This can be expressed in the equivalent algebraic form :\n",
    "\n",
    "\\begin{equation}\n",
    "b + \\sum_{j=1}^{n} w_j x_j\n",
    "\\end{equation}\n",
    "\n",
    "Or in more compact matrix notation :\n",
    "\n",
    "\\begin{equation}\n",
    "b + \\mathbf{w}^\\mathsf{T}\\mathbf{x}\n",
    "\\end{equation}\n",
    "\n",
    "We note that the addition term on the right is actually the [dot product](https://en.wikipedia.org/wiki/Dot_product) of the vectors in feature and weight spaces. Here are some important characteristics of the dot product :\n",
    "* It is zero when the two vectors are [orthogonal](https://en.wikipedia.org/wiki/Orthogonality) to each other.\n",
    "* It is a maximum when the two vectors point in exactly the same direction.\n",
    "* It is a minimum when the two vectors point in completely opposite directions.\n",
    "\n",
    "Finally, as a useful visual, we can also imagine the following data flow representation of the linear expression :\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/LinearEquation.png?raw=true\">\n",
    "\n",
    "Here, we can clearly see that this expression is the sum of weighted values of the features plus a bias, hence the names for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "In linear regression, we are interested in mapping features to a numeric target. The linear expression in the previous section is directly used as the hypothesis function.\n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\mathbf{p}}(\\mathbf{x}) = b + w_1x_1 + w_2x_2 + \\cdots + w_nx_n = \\hat{y}\n",
    "\\end{equation}\n",
    "\n",
    "Let us first consider the single-feature case, with $\\mathbf{x}^{(i)} = \\begin{bmatrix} x_1^{(i)} \\end{bmatrix}$. Suppose we plot the feature values against the actual target values, we may get something like this :\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/SimpleLinearRegressionDots.png?raw=true\">\n",
    "\n",
    "The points appear to fall onto an imaginary straight line in the 2-dimensional model space. In such a case, a linear model will provide a good generalization of the data. The algebraic expression for such a straight line is :\n",
    "\n",
    "\\begin{equation}\n",
    "h_(x) = b + w_1x_1 = \\hat{y}\n",
    "\\end{equation}\n",
    "\n",
    "Plotting a line which provides a good fit :\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/SimpleLinearRegression.png?raw=true\">\n",
    "\n",
    "This line is fully defined by the two parameters $b$ and $w_1$. By varying them, different lines of different fits will be obtained. The gradient of the line is proportional to $w_1$. The bias $b$ determines where the line crosses the vertical line $x_1 = 0$. Once we have this linear expression, we are able to predict the value of $y$ (i.e. $\\hat{y}$) for any value of $x_1$ within this range.  \n",
    "\n",
    "In a similar fashion, the idea can be extended to multiple-dimensional feature spaces. For 2-dimensions ($\\mathbf{x}^{(i)} = \\begin{bmatrix} x_1^{(i)} & x_2^{(i)} \\end{bmatrix}^\\top$), the linear expression defines a flat plane tilted in a 3-dimensional model space.\n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\mathbf{p}}(\\mathbf{x}) = b + wx_1 + wx_2 = \\hat{y}\n",
    "\\end{equation}\n",
    "\n",
    "In general, although not possible to imagine in our minds, if we have $n$ dimensions in our feature space, we seek an $n$-dimensional hyperplane that will fit our data plotted in $(n+1)$-dimensional model space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "In logistic regression, we are interested in mapping features to a categorical target. In this section, we only consider binary classes (e.g. positive or negative, true or false). Multi-class classification is simply an extension of the same basic idea.\n",
    "\n",
    "While linear regression attempts to map features to a continuum of target values, for binary class logistic regression it is more restrictive. Only 2 target values are expected, which we will define as 0 and 1. A convenient technique employed to achieve this is to make use of the [Sigmoid function](https://nbviewer.jupyter.org/github/basilhan/math/blob/master/PythonSigmoid.ipynb) shown below for our hypothesis function.\n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\mathbf{p}}(\\mathbf{x}) =\n",
    "\\sigma_{\\mathbf{p}}(\\mathbf{x}) =\n",
    "\\frac{1}{1 + e^{-(b + \\mathbf{w}^\\top\\mathbf{x})}}\n",
    "\\end{equation}\n",
    "\n",
    "Plotting the function along a single dimension with unit weight and zero bias, we observe the below S-shaped curve which transitions from 0 to 1 as $x$ increases :\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/Sigmoid.png?raw=true\">\n",
    "\n",
    "$$\n",
    "\\sigma(x) =\n",
    "\\frac{1}{1 + e^{-(b + wx)}}\n",
    "$$\n",
    "\n",
    "With an additional step, we can convert the sigmoid function output to one of two values as our learned target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "  \\hat{y} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sigma(x) \\leq 0.5 \\\\\n",
    "      1 & \\mbox{if } \\sigma(x) \\gt 0.5\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "This is equivalent to the below expression considering only the exponent :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\hat{y} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } b + wx \\leq 0 \\\\\n",
    "      1 & \\mbox{if } b + wx \\gt 0\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can then easily extend this to multiple dimensions :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\hat{y} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } b + \\mathbf{w}^\\top\\mathbf{x} \\leq 0 \\\\\n",
    "      1 & \\mbox{if } b + \\mathbf{w}^\\top\\mathbf{x} \\gt 0\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Below shows the 2-dimensional sigmoid function.\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/Sigmoid2D.png?raw=true\">\n",
    "\n",
    "Although we are not able to imagine for higher dimensions in $n$, as a generalization, the hypercurve defined by the multi-variate sigmoid function in $(n+1)$-dimensional model space projects onto a $(n-1)$-dimensional hyperplane in the feature space. This is in fact our decision hyperplane and is defined by the equation : \n",
    "\n",
    "\\begin{equation}\n",
    "b + w_1x_1 + w_2x_2 + \\cdots + w_nx_n = 0\n",
    "\\end{equation}\n",
    "\n",
    "There are several properties to note.  \n",
    "\n",
    "* The hyperplane divides the feature space into two halves.\n",
    "* Where the expression on the LHS of the equation evaluates to a positive real number, $\\hat{y} = 1$. Therefore, the set of features for which this is true belongs to the \"1\" class.\n",
    "* Similarly, the set of features for which the LHS expression evaluates to a negative real number belongs to the \"0\" class ($\\hat{y} = 0$).\n",
    "* The bias $b$ regulates the hyperplane's distance from the origin. Increasing the bias shifts the hyperplane such that more of the feature space falls under the \"1\" class and vice versa.\n",
    "* The direction pointed to by the weight vector $\\mathbf{w}$ is always orthogonal to the hyperplane. In other words, \n",
    "$\\mathbf{w}$ regulates the orientation of (because it is orthogonal to) the decision hyperplane. This interesting result is an implication of the dot product mentioned in the previous section on linear models.\n",
    "\n",
    "There are more details on the effect of $b$ and $\\mathbf{w}$ available in the [Sigmoid function](https://nbviewer.jupyter.org/github/basilhan/math/blob/master/PythonSigmoid.ipynb) section.\n",
    "\n",
    "For illustrative purpose, let us consider a feature space of 2-dimensions ($\\mathbf{x}^{(i)} = \\begin{bmatrix} x_1^{(i)} & x_2^{(i)} \\end{bmatrix}^\\top$). The decision hyperplane in this case will be a straight line (i.e. dimension of 1). Below is an example which classifies green data instances ($\\hat{y} = 1$) and red data instances ($\\hat{y} = 0$) with the line defined by the equation :\n",
    "\n",
    "$$\n",
    "-0.422 + 2.22x_1 -3.76x_2 = 0\n",
    "$$\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/BinaryClassification.png?raw=true\">\n",
    "\n",
    "This example is taken from a more detailed explanation [here](https://nbviewer.jupyter.org/github/basilhan/ml-in-action/blob/master/PythonBasicBivariateLogisticRegression.ipynb). Note also that the arrow in the plot points in the same direction as the weight vector $\\mathbf{w} = \\begin{bmatrix} 2.22 & -3.76 \\end{bmatrix}^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Permalink : https://nbviewer.jupyter.org/github/basilhan/ml-concepts/blob/master/PythonHypothesisFunction.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
