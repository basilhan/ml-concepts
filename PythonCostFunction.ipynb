{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Index](https://github.com/basilhan/ml-concepts/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Names\n",
    "Loss function  \n",
    "Objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Recall that a machine learning model is described mathematically by its [hypothesis function](https://github.com/basilhan/ml-concepts/blob/master/PythonHypothesisFunction.ipynb) $h_{\\mathbf{p}}(\\mathbf{x})$. This function is parameterized by a set of parameters $\\mathbf{p}$. In other words, there can be as many possible versions of a selected model as the set of all possible permutations of parameters (i.e. the parameter space). Obviously, the versions will be of varying quality. Hence we need to define what makes a version <i>good</i>. It is for this purpose that we introduce the cost function.  \n",
    "\n",
    "The cost function $J(\\mathbf{p})$ is a measure of the errors in the model. Mathematically, given a dataset $\\begin{bmatrix} \\mathbf{x}^{(1)} \\cdots \\mathbf{x}^{(m)} \\end{bmatrix}$, the function maps each version of a model (as defined by the parameters $\\mathbf{p}$) to a positive real number. This is an average score of the deviations of the learned values from the actual values.  \n",
    "\n",
    "$$\n",
    "J:\\mathbf{p} \\in \\mathbb{R}^{n+1} \\mapsto \\mathbb{R} \\text{ for} \\begin{bmatrix} \\mathbf{x}^{(1)} \\cdots \\mathbf{x}^{(m)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There exists a particular set of parameters $\\mathbf{p}_{opt}$ which returns the minimal value of $J(\\mathbf{p})$. We are interested in determining $\\mathbf{p}_{opt}$. But first, we need to define $J(\\mathbf{p})$. This is determined by the specific model selected, to which we will turn our attention next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "Linear regression seeks to map features to a numeric target using a hypothesis function of the form :\n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\mathbf{p}}(\\mathbf{x}) = b + w_1x_1 + w_2x_2 + \\cdots + w_nx_n = \\hat{y}\n",
    "\\end{equation}\n",
    "\n",
    "Since we are dealing with numeric values, it is natural to consider a cost function that is proportional to the absolute difference between $\\hat{y}^{(i)}$ and the actual value $y^{(i)}$ for a typical data instance. We usually define $J(\\mathbf{p})$ as below :\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{p}) = \\frac{1}{2m}\\sum^{m}_{i=1}\\big(h_\\mathbf{p}(\\mathbf{x}^{(i)})-y^{(i)}\\big)^2\n",
    "\\end{equation}\n",
    "\n",
    "Some explanation :\n",
    "* Instead of a simple difference, the square of the difference between $\\hat{y}^{(i)}$ and $y^{(i)}$ is used as it automatically removes any distinction which of the two are greater. In other words, that $\\hat{y}^{(i)}$ is greater than $y^{(i)}$ by a certain value is treated as the same amount of deviation as $\\hat{y}^{(i)}$ smaller than $y^{(i)}$ by the same value, since the square is the same positive value. The squaring also gives much greater weight to larger deviations since the square function is not linear.\n",
    "* All the squared differences are summed up across the dataset $\\begin{bmatrix} \\mathbf{x}^{(1)} \\cdots \\mathbf{x}^{(m)} \\end{bmatrix}$.\n",
    "* The $\\frac{1}{2m}$ term completes the mean calculation. The half is added to make later calculations more elegant.\n",
    "\n",
    "The plot of a typical cost function for a 2-dimensional parameter space can be seen below, shown as a smooth surface.\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/LinearRegressionCostFunction.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "In logistic regression, features are mapped to a categorical target. Just as we did in the discussion on [hypothesis function](https://github.com/basilhan/ml-concepts/blob/master/PythonHypothesisFunction.ipynb), we only consider binary classes. In this case, we have the classes \"0\" and \"1\".  \n",
    "\n",
    "Customarily, we use the following cost function which is derived from statistics using the principle of [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation):\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{p}) = -\\frac{1}{m}\\sum^{m}_{i=1} \\bigg[ y^{(i)} \\log \\big(h_\\mathbf{p}(\\mathbf{x}^{(i)})\\big) + ( 1 - y^{(i)}) \\log \\big(1-h_\\mathbf{p}(\\mathbf{x}^{(i)})\\big) \\bigg]\n",
    "\\end{equation}\n",
    "\n",
    "Let us unpack this expression with the help of the below plots of the log function against $h_{\\mathbf{p}}(\\mathbf{x})$.\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/LogisticRegressionCostFunction.png?raw=true\">\n",
    "\n",
    "* Distribute the minus sign outside the summation over the two addition terms inside the square brackets. You will have the two functions in the plot.\n",
    "* One of the two addition terms is always zero for any data instance $\\mathbf{x}^{(i)}$. If $y^{(i)} = 0$, the left term is zero and we are only concerned with the right term. If $y^{(i)} = 1$, the right term is zero and we are only concerned with the left term.\n",
    "* [Recall]((https://github.com/basilhan/ml-concepts/blob/master/PythonHypothesisFunction.ipynb) that $h_{\\mathbf{p}}(\\mathbf{x}^{(i)})$ is only defined in the range $[0,1]$. Values less than or equal to $0.5$ correspond to $y^{(i)} = 0$ and values greater than $0.5$ correspond to $y^{(i)} = 1$.\n",
    "* Although there is a discrete jump in the error rate at the threshold of $0.5$ as the prediction \"flips\", we desire a smooth function for mathematical tractability. We also want to penalize more the further the value of $h_{\\mathbf{p}}(\\mathbf{x}^{(i)})$ is from either $0$ or $1$ depending on $y^{(i)}$.\n",
    "* If $y^{(i)} = 0$, we follow the yellow line in the plot. For values of $h_{\\mathbf{p}}(\\mathbf{x}^{(i)})$ close to $0$, the cost is small. This increases rapidly as $h_{\\mathbf{p}}(\\mathbf{x}^{(i)})$ gets closer to $1$.\n",
    "* If $y^{(i)} = 1$, we follow the blue line in the plot. For values of $h_{\\mathbf{p}}(\\mathbf{x}^{(i)})$ close to $1$, the cost is small. This increases rapidly as $h_{\\mathbf{p}}(\\mathbf{x}^{(i)})$ gets closer to $0$.\n",
    "* The $\\frac{1}{m}$ term completes the mean calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locating the Minimum of the Cost Function\n",
    "\n",
    "So far we have defined our $J(\\mathbf{p})$ but our work is only half done. We seek $\\mathbf{p}_{opt}$ which returns the minimal value. We could try out different parameters randomly and observe what $J(\\mathbf{p})$ returns and then choose the one which is smallest. But considering the size of the parameter space in a typical machine learning task, locating the optimal set by chance would be hugely improbable. It would be good to have a more systematic way of accomplishing this. A popular technique employed is the gradient descent algorithm which is described [here](https://github.com/basilhan/ml-concepts/blob/master/PythonGradientDescent.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Permalink : https://nbviewer.jupyter.org/github/basilhan/ml-concepts/blob/master/PythonCostFunction.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
