{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Index](https://github.com/basilhan/ml-concepts/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "A hypothesis function $h_{\\mathbf{p}}(\\mathbf{x})$ provides a mathematical description of a machine learning model by mapping the feature vector $\\mathbf{x}$ to a real value that will eventually be converted to the final learned target value. The subscript in $h_{\\mathbf{p}}(\\mathbf{x})$ indicates that the function is parameterized by the parameter vector $\\mathbf{p}$.\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/HypothesisFunction.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models\n",
    "\n",
    "Many types of machine learning models are [linearly](https://en.wikipedia.org/wiki/Linear_function) parameterized. In these models, the parameter vector consists of a a scalar $b$ known as the bias, and a weight vector $\\mathbf{w}$. Mathematically, the parameter vector can be expressed as :\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{p}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        p_0 \\\\\n",
    "        p_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        p_n\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        b \\\\\n",
    "        w_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        w_n\n",
    "    \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "where $b = p_0$ and $w_j = p_j$ for $j = 1, 2, \\cdots, n$. Therefore, the weight vector, when fully expanded, is :\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{w}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        w_1 \\\\\n",
    "        w_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        w_n\n",
    "    \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the value of $n$ is also the number of dimensions in the feature space. There is therefore a one-to-one correspondence between the weight vector and the feature vector $\\begin{bmatrix} x_1 & x_2 & \\cdots & x_n \\end{bmatrix}^\\top$. That is, the weight $w_j$ corresponds to the feature $x_j$, where $j$ is an integer from $1$ to $n$. We can now write out the following linear expression.\n",
    "\n",
    "$$\n",
    "b + w_1x_1 + w_2x_2 + \\cdots + w_nx_n\n",
    "$$\n",
    "\n",
    "This can be expressed in the equivalent algebraic form :\n",
    "\n",
    "$$\n",
    "b + \\sum_{j=1}^{n} w_j x_j\n",
    "$$\n",
    "\n",
    "Or in more compact matrix notation :\n",
    "\n",
    "$$\n",
    "b + \\mathbf{w}^\\mathsf{T}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "We note that the addition term on the right is actually the [dot product](https://en.wikipedia.org/wiki/Dot_product) of the vectors in feature and weight spaces. Here are some important characteristics of the dot product :\n",
    "* It is zero when the two vectors are [orthogonal](https://en.wikipedia.org/wiki/Orthogonality) to each other.\n",
    "* It is a maximum when the two vectors point in exactly the same direction.\n",
    "* It is a minimum when the two vectors point in completely opposite directions.\n",
    "\n",
    "Finally, as a useful visual, we can also imagine the following data flow representation of the linear expression :\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/LinearEquation.png?raw=true\">\n",
    "\n",
    "Here, we can clearly see that this expression is the sum of weighted values of the features plus a bias, hence the names for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "In linear regression, we are interested in mapping features to a numeric target. The linear expression in the previous section is directly used as the hypothesis function.\n",
    "\n",
    "$$\n",
    "h_{\\mathbf{p}}(\\mathbf{x}) = b + w_1x_1 + w_2x_2 + \\cdots + w_nx_n = \\hat{y}\n",
    "$$\n",
    "\n",
    "Let us first consider the single-feature case, with $\\mathbf{x}^{(i)} = \\begin{bmatrix} x_1^{(i)} \\end{bmatrix}$. Suppose we plot the feature values against the actual target values, we may get something like this :\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/SimpleLinearRegressionDots.png?raw=true\">\n",
    "\n",
    "The points appear to fall onto an imaginary straight line in the 2-dimensional model space. In such a case, a linear model will provide a good generalization of the data. The algebraic expression for such a straight line is :\n",
    "\n",
    "$$\n",
    "h_{\\mathbf{p}}(\\mathbf{x}) = b + wx_1 = \\hat{y}\n",
    "$$\n",
    "\n",
    "Plotting a line which provides a good fit :\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/SimpleLinearRegression.png?raw=true\">\n",
    "\n",
    "This line is fully defined by the two parameters $b$ and $w$. By varying them, different lines of different fits will be obtained. The gradient of the line is proportional to $w_1$. The bias $b$ determines where the line crosses the vertical line $x_1 = 0$. Once we have this linear expression, we are able to predict the value of $y$ (i.e. $\\hat{y}$) for any $x_1$ within this range.  \n",
    "\n",
    "In a similar fashion, the idea can be extended to multiple-dimensional feature spaces. For 2-dimensions ($\\mathbf{x}^{(i)} = \\begin{bmatrix} x_1^{(i)} & x_2^{(i)} \\end{bmatrix}^\\top$), the linear expression defines a flat plane tilted in a 3-dimensional model space.\n",
    "\n",
    "$$\n",
    "h_{\\mathbf{p}}(\\mathbf{x}) = b + wx_1 + wx_2 = \\hat{y}\n",
    "$$\n",
    "\n",
    "In general, although not possible to imagine in our minds, if we have $n$ dimensions in our feature space, we seek an $n$-dimensional hyperplane that will fit our data plotted in $(n+1)$-dimensional model space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "In logistic regression, we are interested in mapping features to a categorical target. In this section, we only consider binary classes (e.g. positive or negative, true or false). Multi-class classification is simply an extension of the same basic idea.\n",
    "\n",
    "While linear regression attempts to map features to a continuum of target values, for binary class logistic regression it is more restrictive. Only 2 target values are expected, which we will define as 0 and 1. A convenient technique employed to achieve this is to make use of the [Sigmoid function](https://nbviewer.jupyter.org/github/basilhan/math/blob/master/PythonSigmoid.ipynb) shown below for our hypothesis function.\n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\mathbf{p}}(\\mathbf{x}) =\n",
    "\\sigma_{\\mathbf{p}}(\\mathbf{x}) =\n",
    "\\frac{1}{1 + e^{-(b + \\mathbf{w}^\\top\\mathbf{x})}}\n",
    "\\end{equation}\n",
    "\n",
    "Plotting the function along a single dimension with unit weight and zero bias, we observe the below S-shaped curve which transitions from 0 to 1 :\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/Sigmoid.png?raw=true\">\n",
    "\n",
    "$$\n",
    "\\sigma(x) =\n",
    "\\frac{1}{1 + e^{-(b + wx)}}\n",
    "$$\n",
    "\n",
    "With an additional step, we can convert the sigmoid function output to one of two values as our learned target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "  \\hat{y} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sigma(x) \\leq 0.5 \\\\\n",
    "      1 & \\mbox{if } \\sigma(x) \\gt 0.5\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "This is equivalent to the below :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\hat{y} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } b + wx \\leq 0 \\\\\n",
    "      1 & \\mbox{if } b + wx \\gt 0\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can easily extend this to multiple dimensions :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\hat{y} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } b + \\mathbf{w}^\\top\\mathbf{x} \\leq 0 \\\\\n",
    "      1 & \\mbox{if } b + \\mathbf{w}^\\top\\mathbf{x} \\gt 0\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, intimately connected to the value of $\\hat{y}$ is the expression : \n",
    "\n",
    "$$\n",
    "b + w_1x_1 + w_2x_2 + \\cdots + w_nx_n\n",
    "$$\n",
    "\n",
    "If we have known values for $b$ and $w_j$ for $j = 1, 2, \\cdots, n$, \n",
    "\n",
    "This is equivalent to solving for a $(n-1)$-dimensional decision hyperplane within a $n$-dimensional space.  \n",
    "\n",
    "For example, if $n = 2$, the decision hyperplane is 1-dimensional line in a 2-dimensional space. Below shows the line for the function\n",
    "\n",
    "$$\n",
    "-0.422 + 2.22x_1 -3.76x_2 = 0\n",
    "$$\n",
    "\n",
    "This example is taken from a more detailed explanation [here](https://nbviewer.jupyter.org/github/basilhan/ml-in-action/blob/master/PythonBasicBivariateLogisticRegression.ipynb).\n",
    "\n",
    "<img src=\"https://github.com/basilhan/figures/blob/master/BinaryClassification.png?raw=true\">\n",
    "\n",
    "If $n = 3$, we have a 2-dimensional plane in 3-dimensional space, and so on. In every case, we have a hyperplane which bisects the space into a positive half and a negative half.  \n",
    "\n",
    "There are some interesting properties associated with the weight vector $\\mathbf{w}$ and the bias $b$. The direction pointed to by $\\mathbf{w}$ is always orthogonal to the hyperplane. This is illustrated by the arrow in the 2-dimensional plot above. In other words, $\\mathbf{w}$ regulates the orientation of (because it is orthogonal to) the decision hyperplane. Independently, the bias $b$ regulates the hyperplane's distance from the origin. More mathematical details can be found [here](https://nbviewer.jupyter.org/github/basilhan/math/blob/master/PythonSigmoid.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Permalink : https://nbviewer.jupyter.org/github/basilhan/ml-concepts/blob/master/PythonHypothesisFunction.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
